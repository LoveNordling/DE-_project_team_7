{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "quiet-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "import timeit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "negative-multimedia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Session\n"
     ]
    }
   ],
   "source": [
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.131:7077\") \\\n",
    "        .appName(\"experiment_2.ipynb\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.cores.max\",2)\\\n",
    "        .config(\"spark.worker.instances\",5)\\\n",
    "        .config(\"spark.executor.cores\",1)\\\n",
    "        .config(\"spark.driver.port\",9998)\\\n",
    "        .config(\"spark.blockManager.port\",10005)\\\n",
    "        .getOrCreate()\n",
    "#\\#)\\\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "\n",
    "spark_context.setLogLevel(\"INFO\")\n",
    "print(\"Started Session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "requested-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_dataframe(filename):\n",
    "    df = spark_session.read.json('hdfs://192.168.2.131:9000/user/ubuntu/{}'.format(filename))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "weekly-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- author: string (nullable = true)\n",
      " |-- author_flair_css_class: string (nullable = true)\n",
      " |-- author_flair_text: string (nullable = true)\n",
      " |-- body: string (nullable = true)\n",
      " |-- controversiality: long (nullable = true)\n",
      " |-- created_utc: long (nullable = true)\n",
      " |-- distinguished: string (nullable = true)\n",
      " |-- edited: boolean (nullable = true)\n",
      " |-- gilded: long (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- link_id: string (nullable = true)\n",
      " |-- parent_id: string (nullable = true)\n",
      " |-- retrieved_on: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- stickied: boolean (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- subreddit_id: string (nullable = true)\n",
      " |-- ups: long (nullable = true)\n",
      "\n",
      "execution time 17.098277665965725s\n"
     ]
    }
   ],
   "source": [
    "start_time_0 = timeit.default_timer() \n",
    "\n",
    "df = create_spark_dataframe('RC_2005-12.json')\n",
    "# df = create_spark_dataframe('RC_2011-08.json')\n",
    "# df = create_spark_dataframe('RC_2009-05.json')\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "print(\"execution time {}s\".format(timeit.default_timer()-start_time_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stunning-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df.select(\"body\").withColumnRenamed(\"body\", \"comment\").rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "injured-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(comment='A look at Vietnam and Mexico exposes the myth of market liberalisation.'),\n",
       " Row(comment='The site states \"What can I use it for? Meeting notes, Reports, technical specs Sign-up sheets, proposals and much more...\", just like any other new breeed of sites that want us to store everything we have on the web. And they even guarantee multiple levels of security and encryption etc. But what prevents these web site operators fom accessing and/or stealing Meeting notes, Reports, technical specs Sign-up sheets, proposals and much more, for competitive or personal gains...? I am pretty sure that most of them are honest, but what\\'s there to prevent me from setting up a good useful site and stealing all your data? Call me paranoid - I am.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "molecular-trust",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = comments.map(lambda x: [x[0].split()]).toDF([\"comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rational-latvia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(comment=['A', 'look', 'at', 'Vietnam', 'and', 'Mexico', 'exposes', 'the', 'myth', 'of', 'market', 'liberalisation.']),\n",
       " Row(comment=['The', 'site', 'states', '\"What', 'can', 'I', 'use', 'it', 'for?', 'Meeting', 'notes,', 'Reports,', 'technical', 'specs', 'Sign-up', 'sheets,', 'proposals', 'and', 'much', 'more...\",', 'just', 'like', 'any', 'other', 'new', 'breeed', 'of', 'sites', 'that', 'want', 'us', 'to', 'store', 'everything', 'we', 'have', 'on', 'the', 'web.', 'And', 'they', 'even', 'guarantee', 'multiple', 'levels', 'of', 'security', 'and', 'encryption', 'etc.', 'But', 'what', 'prevents', 'these', 'web', 'site', 'operators', 'fom', 'accessing', 'and/or', 'stealing', 'Meeting', 'notes,', 'Reports,', 'technical', 'specs', 'Sign-up', 'sheets,', 'proposals', 'and', 'much', 'more,', 'for', 'competitive', 'or', 'personal', 'gains...?', 'I', 'am', 'pretty', 'sure', 'that', 'most', 'of', 'them', 'are', 'honest,', 'but', \"what's\", 'there', 'to', 'prevent', 'me', 'from', 'setting', 'up', 'a', 'good', 'useful', 'site', 'and', 'stealing', 'all', 'your', 'data?', 'Call', 'me', 'paranoid', '-', 'I', 'am.'])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "right-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2Vec = Word2Vec(vectorSize=4, minCount=4, inputCol=\"comment\", outputCol=\"result\")\n",
    "model = word2Vec.fit(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "chief-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms = model.findSynonyms('pretty', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "experimental-oracle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fact 0.9996135830879211\n",
      "it, 0.9976227283477783\n",
      "thanks 0.9950999617576599\n",
      "movies 0.9936641454696655\n",
      "new 0.9933768510818481\n"
     ]
    }
   ],
   "source": [
    "for word, cosine_distance in synonyms.collect():\n",
    "    print(word, cosine_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "enormous-hearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:py4j.java_gateway:An error occurred while trying to connect to the Java server (127.0.0.1:43105)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 977, in _get_connection\n",
      "    connection = self.deque.pop()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/py4j/java_gateway.py\", line 1115, in start\n",
      "    self.socket.connect((self.address, self.port))\n",
      "ConnectionRefusedError: [Errno 111] Connection refused\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/context.py:460: RuntimeWarning: Unable to cleanly shutdown Spark JVM process. It is possible that the process has crashed, been killed or may also be in a zombie state.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ended Session\n"
     ]
    }
   ],
   "source": [
    "# release the cores for another application!\n",
    "spark_context.stop()\n",
    "print(\"Ended Session\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-contract",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
